{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYYjrbod7F3V"
   },
   "source": [
    "# TP BERT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJX8UgsoZMtQ"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drnebPZ26_yA"
   },
   "source": [
    "In this lab, we will:\n",
    "\n",
    "0. Understand how to use a *Jupyter Notebook*, a very popular tool in data analysis  \n",
    "1. Prepare a dataset for training a [BERT](https://arxiv.org/abs/1810.04805) model  \n",
    "3. Understand how to use a pre-trained BERT model  \n",
    "4. Create a multi-class classification model that leverages the hidden representations of a BERT encoder  \n",
    "5. Train this model and test its performance  \n",
    "\n",
    "With the following tools:  \n",
    "1. [PyTorch](https://pytorch.org/docs/stable/index.html): an *open-source* Python library for machine learning  \n",
    "2. [HuggingFace’s Transformers](https://huggingface.co/transformers/): a PyTorch-based library for natural language processing, especially Transformer models (like BERT)  \n",
    "3. [HuggingFace’s Tokenizers](https://github.com/huggingface/tokenizers): a PyTorch-based library for tokenization, explicitly designed to work with the Transformers library  \n",
    "4. Google Colab, which hosts this *Jupyter Notebook*. Before starting the lab, you can review introductory pages on [Colab](https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l01c01_introduction_to_colab_and_python.ipynb#scrollTo=YHI3vyhv5p85) and [Notebooks](https://realpython.com/jupyter-notebook-introduction/)\n",
    "\n",
    "Colab machines come with a Linux system and a Python environment with several pre-installed libraries like PyTorch. However, if you're using a Colab machine, you must first install the HuggingFace libraries, which are not pre-installed.\n",
    "\n",
    "To run a Unix command on a Jupyter Notebook, you need to prefix the command with an exclamation mark: `!command`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvdKhj2zlq03"
   },
   "outputs": [],
   "source": [
    "!pip install transformers tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eknnVYcJT7-g"
   },
   "source": [
    "Maintenant nous pouvons importer les bibliothéques principales dont nous aurons besoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rgepNKBa5Bbz"
   },
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQfvlh9lllN-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "\n",
    "# Managing arrays\n",
    "import numpy as np\n",
    "\n",
    "import random, math\n",
    "\n",
    "# Plotting tools:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9aD8i7_XGe9"
   },
   "source": [
    "**GPU**\n",
    "\n",
    "L'apprentissage de notre réseau de neurones requiert beaucoup de calculs matriciels. Pour exécuter ces calculs plus rapidement, il est possible d'utiliser un processeur graphique (*GPU*) p. Si vous êtes sur Colab, vous pouvez utiliser un *GPU* en sélectionnant _Runtime -> Change runtime type -> GPU_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "egeCpaPTEPRA"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  print(\"GPU is available.\")\n",
    "  device = torch.cuda.current_device()\n",
    "else:\n",
    "  print(\"Will work on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBr8HR8WizRt"
   },
   "source": [
    "### To be submitted\n",
    "\n",
    "Each exercise in this lab requires a response either in textual form or as code. Every answer must be written in one or more cells following the statement of each exercise.\n",
    "\n",
    "You will submit a compressed directory named `tp_bert_nom1_nom2.zip` containing the contents of the `tp_bert.zip` directory, where the `tp_bert.ipynb` file has been updated with your responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3itFE_0tZIBv"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbHy5DlHllOf"
   },
   "source": [
    "We will train our model on a **multi-class classification task**. Specifically, the task is to classify texts into three sentiment categories:\n",
    "\n",
    "1. Negative\n",
    "2. Neutral\n",
    "3. Positive\n",
    "\n",
    "These data are collected in the [FinancialPhraseBank-v1.0 dataset](https://www.researchgate.net/publication/251231364_FinancialPhraseBank-v10), which you can find in the directory of this lab.\n",
    "\n",
    "Here is the essential information to understand the task, as outlined in the README.txt file:\n",
    "\n",
    "---\n",
    "\n",
    "<em>The key arguments for the low utilization of statistical techniques in financial sentiment analysis have been the difficulty of implementation for practical applications and the lack of high quality training data for building such models. Especially in the case of finance and economic texts, annotated collections are a scarce resource and many are reserved for proprietary use only. To resolve the missing training data problem, we present a collection of ∼ 5000 sentences to establish human-annotated standards for benchmarking alternative modeling techniques.\n",
    "\n",
    "<em>The objective of the phrase level annotation task was to classify each example sentence into a positive, negative or neutral category by considering only the information explicitly available in the given sentence. Since the study is focused only on financial and economic domains, the annotators were asked to consider the sentences from the view point of an investor only; i.e. whether the news may have positive, negative or neutral influence on the stock price. As a result, sentences which have a sentiment that is not relevant from an economic or financial perspective are considered neutral.\n",
    "\n",
    "<em>This release of the financial phrase bank covers a collection of 4840 sentences. The selected collection of phrases was annotated by 16 people with adequate background knowledge on financial markets. Three of the annotators were researchers and the remaining 13 annotators were master’s students at Aalto University School of Business with majors primarily in finance, accounting, and economics.\n",
    "\n",
    "<em>Given the large number of overlapping annotations (5 to 8 annotations per sentence), there are several ways to define a majority vote based gold standard. To provide an objective comparison, we have formed 4 alternative reference datasets based on the strength of majority agreement:\n",
    "\n",
    "1. sentences with 100% agreement [file=Sentences_AllAgree.txt];\n",
    "2. sentences with more than 75% agreement [file=Sentences_75Agree.txt];\n",
    "3. sentences with more than 66% agreement [file=Sentences_66Agree.txt]; and\n",
    "4. sentences with more than 50% agreement [file=Sentences_50Agree.txt].\n",
    "\n",
    "<em>All reference datasets are included in the release. The files are in a machine-readable \"@\"-separated format:\n",
    "\n",
    "<em>**sentence@sentiment**\n",
    "\n",
    "<em>where sentiment is either \"positive, neutral or negative\".\n",
    "\n",
    "<em>E.g.,  The operating margin came down to 2.4 % from 5.7 % .@negative<em>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XpAb_zzTr-71"
   },
   "outputs": [],
   "source": [
    "## Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd drive/MyDrive/TP_BERT/\n",
    " ## your drive folder here up there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-Xmt1rCMZTp"
   },
   "source": [
    "#### Exercise 1\n",
    "\n",
    "We will use the sentences in _Sentences_75Agree.txt_ to train and test our model. To do this, download the file to the Colab machine (using the interface on the left) or, if you are working on a local _Jupyter Notebook_, place the _Sentences_75Agree.txt_ file in the same directory as the _Notebook_.\n",
    "\n",
    "Next, write the `load_data()` function, which reads the sentences contained in this file and separates them from their labels. The result of this function, assigned to the variable `df_data`, should be a dataframe, where each entry contains a sentence as the first element and its label as the second element. For example:\n",
    "\n",
    "````\n",
    "data.iloc[0][0] == 'A high court in Finland has fined seven local asphalt companies more than   lion ( $ 117 million ) for operating a cartel .'\n",
    "data.iloc[0][1] == 'negative'\n",
    "````\n",
    "\n",
    "> Note: The encoding of the _Sentences_75Agree.txt_ file is `iso-8859-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_RUyKYiCPrUT"
   },
   "outputs": [],
   "source": [
    "# Function to load FinancialPhraseBank data\n",
    "# Each line has the form: <sentence>@<label>\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(filename, classes):\n",
    "    \"\"\"Load a FinancialPhraseBank 'Sentences_*.txt' file.\n",
    "\n",
    "    Args:\n",
    "        filename (str): path to the txt file.\n",
    "        classes (list[str]): allowed labels (e.g., ['negative','neutral','positive']).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: two columns [0]=sentence, [1]=label\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with open(filename, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # split on the last '@' (sentences may contain '@' rarely)\n",
    "            if '@' not in line:\n",
    "                continue\n",
    "            sent, lab = line.rsplit('@', 1)\n",
    "            lab = lab.strip()\n",
    "            if lab in classes:\n",
    "                sentences.append(sent.strip())\n",
    "                labels.append(lab)\n",
    "    return pd.DataFrame({0: sentences, 1: labels})\n",
    "\n",
    "# Choose one of the provided datasets (66/75/AllAgree)\n",
    "filename = \"FinancialPhraseBank-v1.0/Sentences_75Agree.txt\"\n",
    "classes = ['negative', 'neutral', 'positive']\n",
    "\n",
    "df_data = load_data(filename, classes)\n",
    "df_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gdz1-MmsO72_"
   },
   "source": [
    "Test your code by running it on the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ws2gU_wEOp8S"
   },
   "outputs": [],
   "source": [
    "assert type(df_data.iloc[0][0]) == str, \"The first column should be a sentence.\"\n",
    "assert df_data.iloc[0][1] in classes, \"The second column should belong to one of the three classes available.\"\n",
    "assert len(df_data) == 3453, \"The size of dataframe should be of 3453 sentences.\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJQ_PsuiQpcp"
   },
   "source": [
    "Now, we will split our data for training in order to test our model.\n",
    "**Question** : Write a function that assigns a Split column (see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z1LjVDtsathc"
   },
   "outputs": [],
   "source": [
    "df_data.iloc[2].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VjHDdloTtrPP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def assign_train_test_split(df, train_size=0.80, seed=42):\n",
    "    \"\"\"Create a 'Split' column with values 'Train' or 'Test'.\n",
    "\n",
    "    The assignment is random but reproducible with the seed.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    mask = rng.random(len(df)) < train_size\n",
    "    df.loc[:, 'Split'] = np.where(mask, 'Train', 'Test')\n",
    "    return df\n",
    "\n",
    "assign_train_test_split(df_data)\n",
    "df_data['Split'].value_counts()\n",
    "\n",
    "# Add numeric label column:\n",
    "label_map = {'neutral': 0, 'positive': 1, 'negative': 2}\n",
    "df_data.loc[:, 'label'] = df_data[1].map(label_map).astype(int)\n",
    "df_data[['Split', 1, 'label']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3cFtY13ruFLH"
   },
   "outputs": [],
   "source": [
    "#Did your function work?\n",
    "assert(np.fabs(len(df_data[df_data[\"Split\"]==\"Train\"])-len(df_data)*0.8) < 100)\n",
    "assert(np.fabs(len(df_data[df_data[\"Split\"]==\"Test\"])-len(df_data)*0.2) < 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGeHg8Enb-Yw"
   },
   "outputs": [],
   "source": [
    "df_data.value_counts(\"Split\", normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypuFm78gllPM"
   },
   "source": [
    "#### Exercise 2\n",
    "\n",
    "Before a classification task, it’s always a good idea to check that the training and test data are (more or less) evenly distributed concerning the existing classes.\n",
    "\n",
    "1. Why?\n",
    "\n",
    "2. What is the number of training and test data corresponding to each class?\n",
    "\n",
    "3. Given the distribution of labels in the _test set_, what are the expectations regarding the performance of our model in terms of _accuracy_?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqQUhkuyllPO"
   },
   "outputs": [],
   "source": [
    "# WRITE CODE TO ANSWER THE QUESTIONS HERE,  PRINT ANSWERS OR WRITE THEM IN A TEXT CELL BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RETCPekGaoKX"
   },
   "source": [
    "### Baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fknJN5vWqlO"
   },
   "source": [
    "#### Exercise 3\n",
    "\n",
    "In the cell below, we train a [multiclass naive Bayes classification model](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) to establish a baseline performance. Thus, the goal moving forward will be to outperform this naive model.\n",
    "\n",
    "1. Why do we expect a BERT-type classification model to be more powerful than the naive Bayes model?\n",
    "2. Under what conditions can non-neural models be more advantageous?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OUmD4p3C2KoR"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ElA0PVFr13kx"
   },
   "outputs": [],
   "source": [
    "def get_tfidf_vectors_and_labels(df, split = \"Train\", max_features = 1000):\n",
    "    #new TF-IDF vectorizer considering only the 1000 most common terms\n",
    "    vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "\n",
    "    #compute the TF-IDF vectors for each text\n",
    "    vectorizer.fit(df[df.Split==split][0])\n",
    "    vectors = vectorizer.transform(df[df.Split==split][0])\n",
    "    labels = df[df.Split==split][1]\n",
    "    return vectors.toarray(), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smO4sdvf2Dtz"
   },
   "outputs": [],
   "source": [
    "def do_tfidf_prediction(df, max_features, model = 'nb'):\n",
    "    vectors_train, labels_train = get_tfidf_vectors_and_labels(df, split=\"Train\", max_features = max_features)\n",
    "    vectors_test, labels_test = get_tfidf_vectors_and_labels(df, split=\"Test\", max_features = max_features)\n",
    "\n",
    "    if model == 'nb':\n",
    "      # train a multinomial Naive-Bayes classifier on the tfidf vectors\n",
    "      classifier = MultinomialNB().fit(vectors_train, labels_train)\n",
    "    else:\n",
    "      ## Here is another classifier:\n",
    "      classifier = LogisticRegression().fit(vectors_train, labels_train)\n",
    "\n",
    "    # use classifier to predict the labels of the test set\n",
    "    predicted_train = classifier.predict(vectors_train)\n",
    "    predicted_test = classifier.predict(vectors_test)\n",
    "\n",
    "    f1_train = f1_score(labels_train, predicted_train, average='macro') #or weighted if you'd like!\n",
    "    f1_test = f1_score(labels_test, predicted_test, average='macro') #or weighted if you'd like!\n",
    "    accuracy = (predicted_test == labels_test).sum()/len(predicted_test)\n",
    "\n",
    "    return f1_train, f1_test, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5o37wWti5l3w"
   },
   "outputs": [],
   "source": [
    "f1_train, f1_test, accuracy = do_tfidf_prediction(df_data, max_features = 100)\n",
    "f1_train, f1_test, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RnbcgeTx7D7S"
   },
   "source": [
    "#### Evaluation\n",
    "We have several choices of metric to measure the performance of our classifier. The most obvious is accuracy - simply the number of correct predictions over the number of total predictions. However, this metric is less interesting for unabalanced classes (as we have in this case).\n",
    "\n",
    "**Question**: give an example of a classification problem in which the accuracy metric is misleadingly optimistic.\n",
    "\n",
    "Therefore, in NLP classification problems, we typically use the F1-score, which is the harmonic mean between precision and recall. This gives us a more nuanced picture of our classifier's performance over all the classes. There are several flavours of F1 score, most notably \"macro\" and \"weighted\" - the former weights each class equally, while the latter weights each class according to its prevalence, giving higher weight to more prevalent classes.\n",
    "\n",
    "**Question**: Which type of F1 score would you like to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ttrezYu64rpC"
   },
   "outputs": [],
   "source": [
    "f1_scores_train = []\n",
    "f1_scores_valid = []\n",
    "max_features_array = [5, 10, 50, 100, 200,300,400,500,600,700,800,900,1000]\n",
    "for max_features in max_features_array:\n",
    "    f1_train, f1_valid, acc = do_tfidf_prediction(df_data, max_features = max_features)\n",
    "    f1_scores_train.append(f1_train)\n",
    "    f1_scores_valid.append(f1_valid)\n",
    "\n",
    "plt.plot(np.arange(len(max_features_array)),f1_scores_train,label=\"F1 Train\")\n",
    "plt.plot(np.arange(len(max_features_array)),f1_scores_valid,label=\"F1 Test\")\n",
    "plt.xticks(np.arange(len(max_features_array)),max_features_array)\n",
    "plt.xlabel(\"Number of Features\")\n",
    "plt.ylabel(\"F1 score\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEW0U3ah7heI"
   },
   "source": [
    "**Question**: What is the optimal value for max_features? Evaluate the classifier on the test set for that best value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhPQ-kYZN0Ev"
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vf3eq1vllQK"
   },
   "source": [
    "The HuggingFace library provides tools for tokenizing text data according to the models we will use: [see the docs](https://huggingface.co/transformers/tokenizer_summary.html).\n",
    "\n",
    "In this lab, we will use a pre-trained model called [DistilBERT](https://arxiv.org/pdf/1910.01108.pdf): an encoder with a [Transformer](https://arxiv.org/abs/1706.03762) architecture that is a distilled version of BERT, making it lighter in terms of memory and faster. The authors present it in their paper as follows:\n",
    "\n",
    "<em>As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3vJPFtqb_YAp"
   },
   "outputs": [],
   "source": [
    "x_train = df_data[df_data.Split=='Train'][0].values\n",
    "y_train = df_data[df_data.Split=='Train']['label'].values\n",
    "x_test = df_data[df_data.Split=='Test'][0].values\n",
    "y_test = df_data[df_data.Split=='Test']['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQlbOyzHllQN"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "MAX_LEN = 512\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', padding=True, truncation=True)\n",
    "\n",
    "# let's check out how the tokenizer works\n",
    "for n in range(3):\n",
    "    # tokenize sentences\n",
    "    tokenizer_out = tokenizer(x_train[n])\n",
    "    # convert numerical tokens to alphabetical tokens\n",
    "    encoded_tok = tokenizer.convert_ids_to_tokens(tokenizer_out.input_ids)\n",
    "    # decode tokens back to string\n",
    "    decoded = tokenizer.decode(tokenizer_out.input_ids)\n",
    "    print(tokenizer_out)\n",
    "    print(encoded_tok, '\\n')\n",
    "    print(decoded, '\\n')\n",
    "    print('---------------- \\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mT5wpgrUllQZ"
   },
   "source": [
    "#### Exercise 4\n",
    "\n",
    "1. What does the output of the tokenizer (`tokenizer_out`) represent?\n",
    "2. What are the special tokens introduced by the tokenizer? What is their function?\n",
    "3. Why do some tokens start with ## (for example, ##rea ##der)?\n",
    "4. Note that the tokenizer we are using has been pre-trained. Why does a tokenizer like this require pre-training before its application?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_42B63m-llQp"
   },
   "source": [
    "#### Exercise 5\n",
    "\n",
    "BERT (and DistilBERT) handles sequences with a maximum length of 512 (`MAX_LEN=512). Check if this length is optimal for our task. In other words, verify what the distribution of text lengths is that we need to classify, and whether it would be beneficial to reduce MAX_LEN in order to decrease the processing time of the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "abnwQh5KllQs"
   },
   "outputs": [],
   "source": [
    "# WRITE CODE TO ANSWER THE QUESTION HERE.  PRINT THE ANSWER OR WRITE IT IN A TEXT CELL BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Zpe2gwqiN0P"
   },
   "source": [
    "Now that we understand how the tokenizer works, we can write a Dataset class that will serve us for training and testing. Indeed, the Python classes that handle batch creation and training require a Dataset-type class as input, like the one below.\n",
    "\n",
    "**Note**: Change `MAX_LEN` if you found a value in the previous exercise that is more advantageous in terms of computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HFI7JYSbllSk"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# MAX_LEN =\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_len):\n",
    "        # variables that are set when the class is instantiated\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # select the sentence and its class\n",
    "        sentence = str(self.sentences[item])\n",
    "        label = self.labels[item]\n",
    "        # tokenize the sencence\n",
    "        tokenizer_out = self.tokenizer(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "            )\n",
    "        # return a dictionary with the output of the tokenizer and the label\n",
    "        return  {\n",
    "            'input_ids': tokenizer_out['input_ids'].flatten(),\n",
    "            'attention_mask': tokenizer_out['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "# instantiate two MyDataset objects\n",
    "train_dataset = MyDataset(x_train, y_train, tokenizer, MAX_LEN)\n",
    "test_dataset = MyDataset(x_test, y_test, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EnRvGB-IAfzg"
   },
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8Eyuxv5KKlh"
   },
   "source": [
    "## Getting to know the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-0mxvMC00YP"
   },
   "source": [
    "It’s time to understand how DistilBERT works. Hugging Face offers us several pre-trained DistilBERT models; we will use [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased), which has a deep architecture with 66,362,880 parameters and was pre-trained on the BookCorpus dataset for 90 hours using eight 16 GB GPUs. This will allow us to achieve very good results by attaching a very lightweight classifier (a single layer) on top of DistilBERT, which will do most of the work, that is, encoding our texts. Our classifier will only require training for a few minutes.\n",
    "\n",
    "We can find the names of all the other pre-trained models offered by Hugging Face [at this address](https://huggingface.co/models).\n",
    "\n",
    "To download a pre-trained model, we use the `.from_pretrained` option:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gKUjMtVrDx2D"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel\n",
    "\n",
    "PRE_TRAINED_MODEL_NAME = 'distilbert-base-uncased'\n",
    "\n",
    "distilbert = DistilBertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "distilbert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PS5JItniHzP0"
   },
   "source": [
    "We can encode the text sequences with a forward pass through DistilBERT, which returns the hidden representations from the last layer.\n",
    "\n",
    "**Question**: Please describe the model. What is the number of layer? What is the dimension of those layers?\n",
    "\n",
    "\n",
    "**Note**: We use the `.unsqueeze(0)` function because normally we pass batches to the model, while this time it is just a single element from the *MyDataset* class. By using `.unsqueeze(0)`, we treat it as a batch of size 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_aq1_vqE3kK"
   },
   "outputs": [],
   "source": [
    "first_sent = train_dataset[0]\n",
    "\n",
    "hidden_state = distilbert(\n",
    "    input_ids=first_sent['input_ids'].unsqueeze(0), attention_mask=first_sent['attention_mask'].unsqueeze(0)\n",
    "    )\n",
    "\n",
    "hidden_state[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gxZwaorkI11b"
   },
   "outputs": [],
   "source": [
    "distilbert.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqA1i6275BcJ"
   },
   "source": [
    "**Question**: What is the size of the vocabulary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpNwIOQUKdHj"
   },
   "source": [
    "#### Exercise 6\n",
    "\n",
    "We now have the necessary elements to build a multi-class classification model based on DistilBERT.\n",
    "\n",
    "Regarding the code below:\n",
    "\n",
    "1. What is the function of this code snippet?\n",
    "   ```Python\n",
    "   if freeze_encoder:\n",
    "       for param in self.encoder.parameters():\n",
    "           param.requires_grad = False\n",
    "   ```\n",
    "\n",
    "2. Why do we only keep the hidden representation of the first token of each sequence with the command `pooled_output = hidden_state[:, 0]`?\n",
    "\n",
    "3. Complete the code under `if labels is not None:` in order to calculate the loss function of the model when targets are passed to the `forward` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eELOpWmQFByQ"
   },
   "outputs": [],
   "source": [
    "# See: https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_distilbert.py#L1304"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IPqk1pk6llTM"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertPreTrainedModel\n",
    "\n",
    "PRE_TRAINED_MODEL_NAME = 'distilbert-base-uncased'\n",
    "FREEZE_PRETRAINED_MODEL = True\n",
    "\n",
    "class DistilBertForSentimentClassification(DistilBertPreTrainedModel):\n",
    "    \"\"\"A lightweight DistilBERT classifier (3-way sentiment).\n",
    "\n",
    "    We reuse DistilBERT as an encoder and add a small classification head.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, num_labels, freeze_encoder=False):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        # DistilBERT encoder (pretrained)\n",
    "        self.encoder = DistilBertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "        # Optional: freeze encoder to train only the head\n",
    "        if freeze_encoder:\n",
    "            for p in self.encoder.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # Classification head (similar to HF's DistilBertForSequenceClassification)\n",
    "        self.pre_classifier = nn.Linear(config.dim, config.dim)\n",
    "        self.classifier = nn.Linear(config.dim, num_labels)\n",
    "        self.dropout = nn.Dropout(config.seq_classif_dropout)\n",
    "\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # Encode\n",
    "        outputs = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        hidden_state = outputs.last_hidden_state  # (bs, seq_len, dim)\n",
    "        pooled = hidden_state[:, 0]               # CLS token representation\n",
    "        pooled = self.pre_classifier(pooled)\n",
    "        pooled = nn.ReLU()(pooled)\n",
    "        pooled = self.dropout(pooled)\n",
    "        logits = self.classifier(pooled)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits, outputs.hidden_states, outputs.attentions)\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'logits': logits,\n",
    "            'hidden_states': outputs.hidden_states,\n",
    "            'attentions': outputs.attentions,\n",
    "        }\n",
    "\n",
    "# instantiate model\n",
    "model = DistilBertForSentimentClassification(\n",
    "    config=distilbert.config,\n",
    "    num_labels=len(classes),\n",
    "    freeze_encoder=FREEZE_PRETRAINED_MODEL,\n",
    ")\n",
    "\n",
    "# print info about model's parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('model total params: ', total_params)\n",
    "print('model trainable params: ', trainable_params)\n",
    "print('\\n', model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8X6goCe5BcK"
   },
   "source": [
    "**Question**: what does the line `pooled_output = hidden_state[:, 0]` do? *Hint*: it selects a token. What is specific about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1bjajujllTv"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfrOtIN4NSaK"
   },
   "source": [
    "*Let's train our classifier!*\n",
    "\n",
    "The time has come. Fortunately, *HuggingFace* provides us with the necessary Python class to handle the training: `Trainer`. We pass it the hyperparameters using the `TrainingArguments` class. We will also pass it the metrics we want for model evaluation during the test phase:\n",
    "\n",
    "1. accuracy\n",
    "2. precision\n",
    "3. recall\n",
    "4. f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0W8S-cFL_rsU"
   },
   "outputs": [],
   "source": [
    "# clean logs and results directory from old files\n",
    "# this could be useful if we were running the cells below multiple times\n",
    "!rm -r ./logs ./results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oIsuEFzillTx"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='macro', zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_dir='./logs',\n",
    "    logging_first_step=True,\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=16,\n",
    "    per_device_train_batch_size=8,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_HknicPPbhA"
   },
   "source": [
    "To visualize the evolution of the *loss* throughout the training, we can use *TensorBoard*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1BZrFuTg81Sh"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4NW8wLAllT7"
   },
   "outputs": [],
   "source": [
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U9j6ZBcQllUV"
   },
   "outputs": [],
   "source": [
    "test_results = trainer.predict(test_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QGmN-bxA5BcN"
   },
   "source": [
    "**Question**: complete the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72_BIMte5Q4K"
   },
   "outputs": [],
   "source": [
    "print('Predictions: \\n', test_results.predictions)\n",
    "print('\\nAccuracy: ', test_results.metrics.get('test_accuracy'))\n",
    "print('Precision (macro): ', test_results.metrics.get('test_precision'))\n",
    "print('Recall (macro): ', test_results.metrics.get('test_recall'))\n",
    "print('F1 (macro): ', test_results.metrics.get('test_f1'))\n",
    "print('\\nClasses:', classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HL1A5EzIPzxT"
   },
   "source": [
    "If we did things correctly, the performance (*accuracy*) of our model in classifying sentences it has never seen is around 70.0%. This is not very satisfying when we consider:\n",
    "\n",
    "1. that the class distribution in our test set was not balanced (see Exercise 2)\n",
    "2. that a naïve Bayes classification model achieves similar or even better accuracy (see Exercise 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-f_LKfkRyAR"
   },
   "source": [
    "**Question**: visualize the prediction of the model using a confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T21FtfBO5TBG"
   },
   "source": [
    "### Enhance the performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIiPrE74SRV6"
   },
   "source": [
    "#### Exercise 7\n",
    "\n",
    "1. How can we improve the model's result?\n",
    "2. Retrain the model by tuning the hyperparameters and calculate its performance using the metrics previously used. What _accuracy_ and _F1_ scores do you achieve?\n",
    "\n",
    "**Hint**: consider letting the model improve the sentence encoding..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ssy9fAXU2w4S"
   },
   "outputs": [],
   "source": [
    "# WRITE CODE TO ANSWER THE QUESTION HERE.  PRINT THE ANSWER OR WRITE IT IN A TEXT CELL BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMSR2zq-cbp3"
   },
   "source": [
    "If we are satisfied with the performance of our model, we can save it to use in the next exercise:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FrOnJarvBDf2"
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = './my_model'\n",
    "trainer.save_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8MUjNgvllUg"
   },
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sX0bwUA5RkST"
   },
   "source": [
    "#### Exercise 8\n",
    "\n",
    "Using the model we just saved, predict the class to which the following sentences belong:\n",
    "\n",
    "````\n",
    "  \"CocaCola saw its share price dropping of more than 25% this semester.\",\n",
    "  \"Despite most of the company's sales are taking place in China, the shareholders decided not to relocate the production there.\",\n",
    "  \"This year's profits quintuplicated with respect to the last year.\"\n",
    "````\n",
    "\n",
    "1. What classes did the model predict?\n",
    "2. With what probability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ZXfWabRllUh"
   },
   "outputs": [],
   "source": [
    "# WRITE CODE TO ANSWER THE QUESTION HERE.  PRINT THE ANSWER OR WRITE IT IN A TEXT CELL BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sne9jShhiNae"
   },
   "source": [
    "\n",
    "## Try another model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4uf5_NyiQ41"
   },
   "source": [
    "#### Exercise 9 (bonus)\n",
    "\n",
    "Experiment with other pre-trained models. The list of pre-trained models is available here: https://huggingface.co/models.\n",
    "\n",
    "- What did you try? Leave your code in the cells below.\n",
    "- What did you learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9FVOmYuOYamk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}